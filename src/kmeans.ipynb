{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans Algorithm\n",
    "\n",
    "L'algorithme K-means répond au problème de partitionnement d'un ensemble de données en k groupes (ou clusters) basé sur les caractéristiques des données. Le but est de minimiser la variance intra-cluster et de maximiser la variance inter-cluster, c'est-à-dire que les données au sein d'un même cluster doivent être aussi similaires que possible, tandis que les données appartenant à différents clusters doivent être distinctes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette premiere version sera une version sans numpy, pour tester apres avec potentiellement une version utilisant numpy pour constater la difference de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par définir les étapes de l'algorithme K-means:\n",
    "\n",
    "- Initialisation des centroïdes: Choisir k points aléatoires dans l'espace des données pour représenter les centroïdes des clusters.\n",
    "- Assigner chaque point de données au centroïde le plus proche.\n",
    "- Mettre à jour les centroïdes en calculant la moyenne des points de données de chaque cluster.\n",
    "- Répéter les étapes 2 et 3 jusqu'à ce que les centroïdes ne changent plus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commençons par l'initialisation des centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def distance_euclidienne(point1, point2):\n",
    "    return math.sqrt(sum((p1 - p2) ** 2 for p1, p2 in zip(point1, point2)))\n",
    "\n",
    "#Test de la fonction\n",
    "point1 = [1,2]\n",
    "point2 = [3,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La formule pour calculer la probabilité de choisir un point x comme centroïde est la suivante:\n",
    "\n",
    "$$\n",
    "P(x) = \\frac{D(x)^2}{\\sum_{x \\in X} D(x)^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui est appelé pour le choix du prochain centroid à initialisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def choose_next_centroid(data, centroids):\n",
    "    distances = []\n",
    "    for point in data:\n",
    "        min_distance = float('inf')\n",
    "        for centroid in centroids:\n",
    "            distance = distance_euclidienne(point, centroid)\n",
    "            #print(\"distance entre \", point, \" et \", centroid, \" : \", distance)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "        distances.append(min_distance)\n",
    "    \n",
    "\n",
    "    somme_des_carres = sum(distance**2 for distance in distances) #calcul de la somme des carrés des distances\n",
    "    probabilites = [distance**2 / somme_des_carres for distance in distances] #calcul des probabilités\n",
    "\n",
    "\n",
    "    index_next_centroid = random.choices(range(len(data)), weights=probabilites, k=1)[0] ## on effectue le choix du prochain centroid en fonction des probabilité\n",
    "    return data[index_next_centroid]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on a la fonction qui permet d'initialiser les centroids, ici nous utilisons l'initialisation kmeans++ qui est une amélioration de l'initialisation aléatoire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_plusplus_initialization(data, k):\n",
    "    index_first_centroid = [random.randint(0, len(data) - 1)] # de manière aléatoire\n",
    "    centroids = [data[index_first_centroid[0]]]\n",
    "\n",
    "    for i in range(1, k):\n",
    "        # choix du prochain centroid\n",
    "        next_centroid = choose_next_centroid(data, centroids)\n",
    "        centroids.append(next_centroid)\n",
    "\n",
    "    return centroids\n",
    "\n",
    "#Test de la fonction\n",
    "data = [[1,2], [3,4], [5,6], [7,8], [9,10]]\n",
    "k = 2\n",
    "centroids = kmeans_plusplus_initialization(data, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fonction qui permet d'attribuer des points à des clusters basés sur la proximité des centroïdes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_points_to_clusters(data, centroids):\n",
    "    assignments = []            # stock l'indice du centroide auquel chaque point est assigné\n",
    "    for point in data:\n",
    "        min_distance = float('inf')\n",
    "        cluster_index = None\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            distance = distance_euclidienne(point, centroid)\n",
    "            if distance < min_distance:\n",
    "                min_distance = distance\n",
    "                cluster_index = i\n",
    "        assignments.append(cluster_index)\n",
    "    return assignments\n",
    "\n",
    "assignments = assign_points_to_clusters(data, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après avoir attribuer les points aux clusters, on recalcule les positions des centroides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def update_centroids(data, assignments, k):\n",
    "    cluster_points = defaultdict(list)      # initialise automatiquement toute nouvelle clé avec une liste vide.\n",
    "    for assignment, point in zip(assignments, data):    # regroupe les points par cluster\n",
    "        cluster_points[assignment].append(point)\n",
    "    \n",
    "    new_centroids = []\n",
    "    for i in range(k):\n",
    "        points = cluster_points[i]\n",
    "        if points:  # si le cluster contient des points\n",
    "            new_centroid = [sum(dim)/len(dim) for dim in zip(*points)]\n",
    "        else:       # si k est très grand ou si la distribution des points est très inégale\n",
    "            new_centroid = random.choice(data)\n",
    "        new_centroids.append(new_centroid)\n",
    "    return new_centroids\n",
    "\n",
    "new_centroids = update_centroids(data, assignments, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite on a cette fonction qui teste si l'algorithme a convergé c'est à dire qui test si les centroids ont changé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converge(old_centroids, new_centroids, tolerance=1e-4): #tolérance utilisé dans le kmeans de sklearn\n",
    "    for old, new in zip(old_centroids, new_centroids): #on recupére les anciens et nouveaux centroïdes\n",
    "        distance = 0\n",
    "        for o, n in zip(old, new):\n",
    "            distance += (o - n) ** 2 #on calcule la distance entre les anciens et nouveaux centroïdes\n",
    "\n",
    "        if distance > tolerance ** 2: #si la distance est supérieur à la tolérance\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enfin on peut rassembler toutes ces fonctions pour créer l'algorithme K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_without_numpy(data, k, max_iters = 100):\n",
    "    centroids = kmeans_plusplus_initialization(data, k) #initialisation des centroïdes\n",
    "    for i in range(max_iters):\n",
    "        #print(\"Centroids à l'itération {} :\\n\".format(i), centroids)\n",
    "        assignments = assign_points_to_clusters(data, centroids) #assignation des points aux clusters\n",
    "        new_centroids = update_centroids(data, assignments, k) #mise à jour des centroïdes\n",
    "        if converge(centroids, new_centroids): #si l'algo converge (les centroïdes ne bouge plus)\n",
    "            print(\"Convergence atteinte après {} itérations\".format(i))\n",
    "            break\n",
    "        centroids = new_centroids #si non on met à jour les centroïdes et on continue\n",
    "    return assignments, centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test de l'algorithme sur un jeu de données basique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[random.random() for _ in range(2)] for _ in range(100)]\n",
    "k = 4\n",
    "assignments, centroids = kmeans_without_numpy(data, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de mieux modulariser et organiser les tests, nous allons instancier la classe KMeans_Without_Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kmeans_without_numpy import KMeans_Without_Numpy\n",
    "kmeans_without_numpy = KMeans_Without_Numpy(k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On lance l'execution de l'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments,centroids = kmeans_without_numpy.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisation de l'algorithme K-means avec matplotlib pour mieux voir le resultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm']  \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, centroid in enumerate(centroids):\n",
    "    cluster_points = [data[i] for i in range(len(data)) if assignments[i] == idx]\n",
    "    cluster_points_x = [point[0] for point in cluster_points]\n",
    "    cluster_points_y = [point[1] for point in cluster_points]\n",
    "        \n",
    "    plt.scatter(cluster_points_x, cluster_points_y, c=colors[idx], label=f'Cluster {idx}')\n",
    "        \n",
    "    plt.scatter(centroid[0], centroid[1], c=colors[idx], marker='x', s=200, linewidths=3)\n",
    "\n",
    "plt.title('Visualisation des Clusters et des Centroids')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on test avec la version de sklearn pour comparer les résultats et se situer par rapport à la performance de notre algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "datanp = np.array(data)\n",
    "\n",
    "kmeans_sklearn = KMeans(n_clusters=4, init='k-means++')\n",
    "kmeans_sklearn.fit(datanp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_sklearn = kmeans_sklearn.cluster_centers_\n",
    "labels_sklearn = kmeans_sklearn.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    # Sélectionne les points de données du cluster i\n",
    "    cluster_points = datanp[labels_sklearn == i]\n",
    "    # Trace les points de données\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], label=f'Cluster {i}')\n",
    "    # Trace le centroid\n",
    "    plt.scatter(centroids_sklearn[i, 0], centroids_sklearn[i, 1], c=colors[i], marker='x', s=200, linewidths=3)\n",
    "\n",
    "plt.title('Clustering avec scikit-learn KMeans')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va commencé les tests avancés en utilisant des données générer par la fonction `make_blobs` de la bibliothèque `sklearn.datasets`. Ce sont des données artificielles qui sont habituellement employées pour tester des algorithmes de clustering . On va testé la qualité du clustering de notre version sans numpy avec la version Kmeans de sklearns, pour cela on va utilisé le score de silhouette."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score de silhouette est une mesure de la qualité d'un clustering qui évalue à quel point chaque point d'un cluster est similaire aux points de son propre cluster comparé à ceux des autres clusters. Ce score est particulièrement utile pour déterminer si le nombre de clusters utilisé dans l'analyse est approprié."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour chaque point `i`, le score de silhouette est calculé comme suit :\n",
    "\n",
    "- a(i) : La distance moyenne entre le point `i` et tous les autres points dans le même cluster. Cela mesure à quel point le point `i` est bien intégré dans son cluster.\n",
    "\n",
    "- b(i) : La distance moyenne minimale du point `i` à tous les points d'un autre cluster dont il n'est pas membre. Cela mesure à quel point le point `i` est loin des points du cluster voisin le plus proche.\n",
    "\n",
    "Le score de silhouette pour un point individuel est alors donné par la formule :\n",
    "\n",
    "$$\n",
    "s(i) = \\frac{b(i) - a(i)}{\\max(a(i), b(i))}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Le score de silhouette varie de −1 à +1.\n",
    "- Un score élevé (proche de +1) indique que le point est bien placé à l'intérieur de son cluster et loin des autres clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va utiliser aussi la méthode du coude qui est une technique graphique utilisée pour aider à déterminer le nombre optimal de clusters dans un algorithme de clustering, comme KMeans. Cette méthode consiste à visualiser la variation de la somme des carrés des distances (SSE) entre les points et leurs centroids assignés, en fonction du nombre de clusters utilisés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction va gérer le calcul de la SSE et des scores de silhouette pour les versions customisées et Scikit-Learn de KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(data, k_range):\n",
    "    from kmeans_without_numpy import KMeans_Without_Numpy\n",
    "    from sklearn.cluster import KMeans as SklearnKMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    sse_custom = {}\n",
    "    sse_sklearn = {}\n",
    "    silhouette_custom = {}\n",
    "    silhouette_sklearn = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Notre version de KMeans\n",
    "        kmeans_custom = KMeans_Without_Numpy(k=k, max_iters=1000)\n",
    "        _, centroids_custom = kmeans_custom.fit(data)\n",
    "        sse_custom[k] = sum(min((sum((x - c) ** 2 for x, c in zip(point, centroid)) for centroid in centroids_custom)) for point in data)\n",
    "        assignments_custom = [min(range(k), key=lambda i: sum((x - c) ** 2 for x, c in zip(point, centroids_custom[i]))) for point in data]\n",
    "        silhouette_custom[k] = silhouette_score(data, assignments_custom)\n",
    "        \n",
    "        # Version scikit-learn de KMeans\n",
    "        kmeans_sklearn = SklearnKMeans(n_clusters=k, init='k-means++', max_iter=1000, random_state=0)\n",
    "        kmeans_sklearn.fit(data)\n",
    "        sse_sklearn[k] = kmeans_sklearn.inertia_\n",
    "        silhouette_sklearn[k] = silhouette_score(data, kmeans_sklearn.labels_)\n",
    "    \n",
    "    return sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette fonction gérera le tracé des graphiques pour les métriques calculées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn):\n",
    "\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    # Ceci trace la méthode du coude pour les deux versions de KMeans\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(list(sse_custom.keys()), list(sse_custom.values()), 'bo-', label='Custom KMeans')\n",
    "    plt.plot(list(sse_sklearn.keys()), list(sse_sklearn.values()), 'rx-', label='Scikit-Learn KMeans')\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('SSE')\n",
    "    plt.title('Méthode du coude')\n",
    "    plt.legend()\n",
    "\n",
    "    # Ceci trace le score de silhouette pour les deux versions de KMeans\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(list(silhouette_custom.keys()), list(silhouette_custom.values()), 'bo-', label='Custom KMeans')\n",
    "    plt.plot(list(silhouette_sklearn.keys()), list(silhouette_sklearn.values()), 'rx-', label='Scikit-Learn KMeans')\n",
    "    plt.xlabel('Nombre de clusters')\n",
    "    plt.ylabel('Score de Silhouette')\n",
    "    plt.title('Score de Silhouette par nombre de clusters')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "data, _ = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
    "\n",
    "k_range = range(2, 10)\n",
    "sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn = calculate_metrics(data, k_range)\n",
    "\n",
    "plot_metrics(sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de la génaration de nos données, on a initialisé le paramètre centers à 4, ce qui signifie qu'on gènère nos 300 points autours de 4 centre distincts. On a appliqué KMeans avec `k=4` (où k est le nombre de clusters que l'algorithme doit trouver), c'est pour cela qu'on obtient autour de `k=4` les meilleurs résultats en termes de regroupement des données selon leur proximité aux centres générés initialement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux implémentations semblent offrir des performances similaires en termes de SSE et les scores de silhouette sont presque identiques, suggérant que malgré les différences potentielles en détails d'implémentation, les deux versions produisent des résultats de clustering de qualité similaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va tester maintenant le temps d'execution en moyenne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "from kmeans_without_numpy import KMeans_Without_Numpy\n",
    "\n",
    "\n",
    "def run_custom_kmeans(data):\n",
    "    kmeans_custom = KMeans_Without_Numpy(k=4, max_iters=1000)\n",
    "    start_time = time.time()\n",
    "    assignments_custom, centroids_custom = kmeans_custom.fit(data)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return centroids_custom, execution_time\n",
    "\n",
    "def run_sklearn_kmeans(data):\n",
    "    kmeans_sklearn = SklearnKMeans(n_clusters=4, init='k-means++',max_iter=1000, random_state=None)\n",
    "    start_time = time.time()\n",
    "    kmeans_sklearn.fit(data)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return kmeans_sklearn.cluster_centers_, execution_time\n",
    "\n",
    "def test_stability(data, runs=10):\n",
    "    results_custom = []\n",
    "    results_sklearn = []\n",
    "    times_custom = []\n",
    "    times_sklearn = []\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        centroids_custom, time_custom = run_custom_kmeans(data)\n",
    "        centroids_sklearn, time_sklearn = run_sklearn_kmeans(data)\n",
    "        results_custom.append(centroids_custom)\n",
    "        results_sklearn.append(centroids_sklearn)\n",
    "        times_custom.append(time_custom)\n",
    "        times_sklearn.append(time_sklearn)\n",
    "    \n",
    "    all_times = np.array(times_custom + times_sklearn)\n",
    "    bins = np.linspace(all_times.min(), all_times.max(), 11)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Custom KMeans', 'Scikit-Learn KMeans'], [np.mean(times_custom), np.mean(times_sklearn)], color=['blue', 'red'])\n",
    "    plt.ylabel('Average Execution Time (seconds)')\n",
    "    plt.title('Comparison of Average Execution Time')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(times_custom, bins=bins, alpha=0.7, label='Custom KMeans', color='blue')\n",
    "    plt.hist(times_sklearn, bins=bins, alpha=0.7, label='Scikit-Learn KMeans', color='red')\n",
    "    plt.xlabel('Execution Time (seconds)')\n",
    "    plt.ylabel('Number of Runs')\n",
    "    plt.title('Histogram of Execution Times')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "test_stability(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le temps d'execution de notre version sans numpy est beaucoup plus lente en comparaison avec la version de scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests de la version avec NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics_NP(data, k_range):\n",
    "    from kmeans_np import KMeansNP\n",
    "    from sklearn.cluster import KMeans as SklearnKMeans\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    \n",
    "    sse_custom = {}\n",
    "    sse_sklearn = {}\n",
    "    silhouette_custom = {}\n",
    "    silhouette_sklearn = {}\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Notre version de KMeans avec NumPy\n",
    "        kmeans_custom = KMeansNP(k=k, max_iters=1000)\n",
    "        _, centroids_custom = kmeans_custom.fit(data)\n",
    "        sse_custom[k] = sum(min((sum((x - c) ** 2 for x, c in zip(point, centroid)) for centroid in centroids_custom)) for point in data)\n",
    "        assignments_custom = [min(range(k), key=lambda i: sum((x - c) ** 2 for x, c in zip(point, centroids_custom[i]))) for point in data]\n",
    "        silhouette_custom[k] = silhouette_score(data, assignments_custom)\n",
    "        \n",
    "        # Version scikit-learn de KMeans\n",
    "        kmeans_sklearn = SklearnKMeans(n_clusters=k, init='k-means++',max_iter=1000, random_state=0)\n",
    "        kmeans_sklearn.fit(data)\n",
    "        sse_sklearn[k] = kmeans_sklearn.inertia_\n",
    "        silhouette_sklearn[k] = silhouette_score(data, kmeans_sklearn.labels_)\n",
    "    \n",
    "    return sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X = digits.data\n",
    "k_range = range(2, 10)\n",
    "sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn = calculate_metrics_NP(X, k_range)\n",
    "\n",
    "\n",
    "plot_metrics(sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En terme de performances, on voit qu'on a rien perdu, testons maintenant le temps d'execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans as SklearnKMeans\n",
    "from kmeans_np import KMeansNP\n",
    "\n",
    "\n",
    "def run_custom_kmeans(data):\n",
    "    kmeans_custom = KMeansNP(k=4, max_iters=1000)\n",
    "    start_time = time.time()\n",
    "    assignments_custom, centroids_custom = kmeans_custom.fit(data)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return centroids_custom, execution_time\n",
    "\n",
    "def run_sklearn_kmeans(data):\n",
    "    kmeans_sklearn = SklearnKMeans(n_clusters=4, init='k-means++',max_iter=1000, random_state=None)\n",
    "    start_time = time.time()\n",
    "    kmeans_sklearn.fit(data)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    return kmeans_sklearn.cluster_centers_, execution_time\n",
    "\n",
    "def test_stability_NP(data, runs=10):\n",
    "    results_custom = []\n",
    "    results_sklearn = []\n",
    "    times_custom = []\n",
    "    times_sklearn = []\n",
    "    \n",
    "    for _ in range(runs):\n",
    "        centroids_custom, time_custom = run_custom_kmeans(data)\n",
    "        centroids_sklearn, time_sklearn = run_sklearn_kmeans(data)\n",
    "        results_custom.append(centroids_custom)\n",
    "        results_sklearn.append(centroids_sklearn)\n",
    "        times_custom.append(time_custom)\n",
    "        times_sklearn.append(time_sklearn)\n",
    "    \n",
    "    all_times = np.array(times_custom + times_sklearn)\n",
    "    bins = np.linspace(all_times.min(), all_times.max(), 11)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(['Custom KMeans', 'Scikit-Learn KMeans'], [np.mean(times_custom), np.mean(times_sklearn)], color=['blue', 'red'])\n",
    "    plt.ylabel('Average Execution Time (seconds)')\n",
    "    plt.title('Comparison of Average Execution Time')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(times_custom, bins=bins, alpha=0.7, label='Custom KMeans', color='blue')\n",
    "    plt.hist(times_sklearn, bins=bins, alpha=0.7, label='Scikit-Learn KMeans', color='red')\n",
    "    plt.xlabel('Execution Time (seconds)')\n",
    "    plt.ylabel('Number of Runs')\n",
    "    plt.title('Histogram of Execution Times')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "test_stability_NP(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque qu'il y a un énorme gains en terme de temps d'execution sur le même jeu de données. Cela est dû principalement à l'utilisation de NumPy, il apporte des avantages significatifs en termes de vitesse grâce à la vectorisation, l'optimisation de la gestion de la mémoire, et les optimisations de bas niveau qui accélèrent les calculs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle est aussi plus rapide que la version de scikit-learn. Cela peut etre dû au fait que scikit-learn est plus général et robuste pour une large variété de cas d'utilisation, ce qui peut introduire un surcoût en termes de performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voyons voir maintenant jusqu'a combien de données cette version avec NumPy est plus rapide que la version de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ensemble de données sur des échantillons de vin avec des caractéristiques chimiques.\n",
    "from sklearn.datasets import load_wine\n",
    "wine = load_wine()\n",
    "X1 = wine.data\n",
    "\n",
    "k_range = range(2, 10)\n",
    "sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn = calculate_metrics_NP(X, k_range)\n",
    "plot_metrics(sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn)\n",
    "\n",
    "test_stability_NP(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici en voit que le temps d'execution n'est pas unanime, des fois c'est le notre qui est plus rapide, et d'autre fois c'est celui de scikit-learn. On commance à atteindre les limites où notre version est plus rapide. Mais en termes de performance de clustering, on reste sur des performances similaire entre les deux versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jeu de données encore plus complexe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un ensemble de données d'images de chiffres manuscrits. Il s'agit de données en haute dimension (chaque image est de 8x8 pixels).\n",
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "X2 = digits.data\n",
    "\n",
    "k_range = range(2, 10)\n",
    "sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn = calculate_metrics_NP(X, k_range)\n",
    "plot_metrics(sse_custom, sse_sklearn, silhouette_custom, silhouette_sklearn)\n",
    "\n",
    "test_stability_NP(X2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici en voit que le temps d'execution est unanime, notre version est moins rapide que celle de scikit-learn. Mais en terme de performance de clustering, on reste sur des performances similaire entre les deux versions. Comme on l'a vu précédemment, scikit-learn est plus général et robuste pour une large variété de cas d'utilisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cas pratique sur un jeu de données réel\n",
    "\n",
    "Nous allons maintenant tester sur un jeu de données réel tiré de le site `Kaggle`, il s'agit d'un jeu de données de clients d'un centre commercial. Le jeu de données contient des informations sur le genre, l'âge, le salaire annuel et le score de dépenses des clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('Data/Mall_Customers.csv')\n",
    "df.head()\n",
    "df.isnull().sum()\n",
    "X1 = df[['Age' , 'Spending Score (1-100)']].iloc[: , :].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On commence par la version sans numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_without_np= KMeans_Without_Numpy(k=4, max_iters=1000)\n",
    "assignments_w_np, centroids_w_np = kmeans_without_np.fit(X1)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm']  \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, centroid in enumerate(centroids_w_np):\n",
    "    cluster_points = [X1[i] for i in range(len(X1)) if assignments_w_np[i] == idx]\n",
    "    cluster_points_x = [point[0] for point in cluster_points]\n",
    "    cluster_points_y = [point[1] for point in cluster_points]\n",
    "        \n",
    "    plt.scatter(cluster_points_x, cluster_points_y, c=colors[idx], label=f'Cluster {idx}')\n",
    "        \n",
    "    plt.scatter(centroid[0], centroid[1], c=colors[idx], marker='x', s=200, linewidths=3)\n",
    "\n",
    "plt.title('Visualisation des Clusters et des Centroids')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Version avec numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_np= KMeansNP(k=4, max_iters=1000)\n",
    "assignments_np, centroids_np = kmeans_without_np.fit(X1)\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm']  \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for idx, centroid in enumerate(centroids_np):\n",
    "    cluster_points = [X1[i] for i in range(len(X1)) if assignments_np[i] == idx]\n",
    "    cluster_points_x = [point[0] for point in cluster_points]\n",
    "    cluster_points_y = [point[1] for point in cluster_points]\n",
    "        \n",
    "    plt.scatter(cluster_points_x, cluster_points_y, c=colors[idx], label=f'Cluster {idx}')\n",
    "        \n",
    "    plt.scatter(centroid[0], centroid[1], c=colors[idx], marker='x', s=200, linewidths=3)\n",
    "\n",
    "plt.title('Visualisation des Clusters et des Centroids')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant à titre de comparaison, on va executer la version de scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_sklearn = KMeans(n_clusters=4, init='k-means++')\n",
    "kmeans_sklearn.fit(X1)\n",
    "\n",
    "centroids_sklearn = kmeans_sklearn.cluster_centers_\n",
    "labels_sklearn = kmeans_sklearn.labels_\n",
    "\n",
    "colors = ['r', 'g', 'b', 'm','y']  \n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "centroids_sklearn = kmeans_sklearn.cluster_centers_\n",
    "labels_sklearn = kmeans_sklearn.labels_\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i in range(4):\n",
    "    # Sélectionne les points de données du cluster i\n",
    "    cluster_points = X1[labels_sklearn == i]\n",
    "    # Trace les points de données\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], c=colors[i], label=f'Cluster {i}')\n",
    "    # Trace le centroid\n",
    "    plt.scatter(centroids_sklearn[i, 0], centroids_sklearn[i, 1], c=colors[i], marker='x', s=200, linewidths=3)\n",
    "\n",
    "plt.title('Clustering avec scikit-learn KMeans')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En comparant les trois résultats, on voit que les trois versions offrent des performances de clustering similaires. Cela confirme que notre implémentation de KMeans avec NumPy et sans sont capables de gérer des données réelles et de produire des résultats de clustering de qualité similaire à la version de scikit-learn."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
